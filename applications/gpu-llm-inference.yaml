---
apiVersion: v1
kind: Service
metadata:
  name: llm-inference
  namespace: gpu-workloads
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    name: api
  selector:
    app: llm-inference
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference
  namespace: gpu-workloads
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-inference
  template:
    metadata:
      labels:
        app: llm-inference
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
        - "--model"
        - "mistralai/Mistral-7B-Instruct-v0.2"  # Change to your preferred model
        - "--dtype"
        - "float16"
        - "--max-model-len"
        - "8192"
        ports:
        - containerPort: 8000
          name: api
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: token
              optional: true
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 24Gi
            cpu: 8
          requests:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
        volumeMounts:
        - name: model-cache
          mountPath: /root/.cache
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: llm-model-cache
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-model-cache
  namespace: gpu-workloads
spec:
  accessModes: ["ReadWriteOnce"]
  storageClassName: gpu-storage
  resources:
    requests:
      storage: 200Gi
---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token
  namespace: gpu-workloads
type: Opaque
stringData:
  token: "your-huggingface-token-here"  # Optional: for gated models
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-inference-ingress
  namespace: gpu-workloads
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
  - host: llm.example.com  # Change this!
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-inference
            port:
              number: 8000